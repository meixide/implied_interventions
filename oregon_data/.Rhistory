h0_0
h0_1
h0_0 = mean(Z[W==0])
h0_0 = mean(Z[W==0])
mean(A[W==0])
mean(A[W==1])
mean(Y)
setwd("/Users/cgmeixide/Projects/implied_interventions")
source("data.R")
data <- medicare_loader(4)
Wall <- data[, c("W.zip_msa_list", "W.birthyear_list", "W.female_list", "W.dep_dx_0m")]
W    <- data$W.dep_dx_0m
Z    <- data$treatment
A    <- data$approved_app
Yprim <- data[, 1]
Y    <- ifelse(Yprim == 0, 0, 1)
## --- HAL helpers ------------------------------------------------------------
# Minimal reimplementation to build design matrices for new data given basis list
make_newx_from_basis <- function(basis_list, X_new) {
X_new <- as.matrix(X_new)
n <- nrow(X_new)
B <- matrix(NA_real_, nrow = n, ncol = length(basis_list))
for (j in seq_along(basis_list)) {
info <- basis_list[[j]]
ind <- rep(1, n)
for (k in seq_along(info$cols)) {
ind <- ind * (X_new[, info$cols[k]] >= info$cutoffs[k])
}
B[, j] <- ind
}
B
}
## --- Prob helpers -----------------------------------------------------------
make_h <- function(w, h0_0, h0_1) ifelse(w == 0, h0_0, h0_1)
hfun   <- function(z, f) z * f + (1 - z) * (1 - f)  # P(Z=z | W) in 1-dim form
## --- Libraries --------------------------------------------------------------
library(hal9001)
library(glmnet)
Win <- as.matrix(Wall)
n   <- nrow(Win)
# h(z|w)
hal_w <- enumerate_basis(x = Win, max_degree = 2)
HW    <- make_design_matrix(Win, hal_w, p_reserve = 1)
fitcvw <- cv.glmnet(HW, Z, family = "binomial", intercept = FALSE, standardize = FALSE)
fitw   <- glmnet(HW, Z, family = "binomial", lambda = fitcvw$lambda.min,
intercept = FALSE, standardize = FALSE)
hw <- as.numeric(predict(fitw, newx = HW, type = "response"))
# Q(z,w) = E[Y|Z,W]
hal_zw <- enumerate_basis(x = cbind(Z, Win), max_degree = 2)
HZW    <- make_design_matrix(cbind(Z, Win), hal_zw, p_reserve = 1)
fitcvy <- cv.glmnet(HZW, Y, family = "binomial", intercept = FALSE, standardize = FALSE)
fity   <- glmnet(HZW, Y, family = "binomial", lambda = fitcvy$lambda.min,
intercept = FALSE, standardize = FALSE)
# Wrapper producing Q(Z,W) on demand (compatible with offset fluctuation)
Q_from_fit <- function(Znew, Win_new) {
Xnew <- cbind(Znew, Win_new)
Hnew <- make_newx_from_basis(hal_zw, Xnew)
as.numeric(predict(fity, newx = Hnew, type = "response"))
}
## --- EIF components ---------------------------------------------------------
calc_plugin_eif_terms <- function(Q_model, Y, Z, Win, W, h_star_fun, hw_vec) {
Q_hat   <- Q_model(Z, Win)
clever  <- (h_star_fun(Z, W) / hfun(Z, hw_vec)) * (Y - Q_hat)
Q0_hat  <- Q_model(rep(0, length(Z)), Win)
Q1_hat  <- Q_model(rep(1, length(Z)), Win)
plugin  <- Q0_hat * (1 - h_star_fun(W = W, z = 1)) + Q1_hat * h_star_fun(W = W, z = 1)
list(clever_term = clever, plugin_term = plugin)
}
## --- Grid of target h* ------------------------------------------------------
lgrid <- 10
grid_matrix <- as.matrix(expand.grid(seq(0, 1, length.out = lgrid),
seq(0, 1, length.out = lgrid)))
colnames(grid_matrix) <- c("h_star_0", "h_star_1")
psi     <- numeric(lgrid^2)
sdbound <- numeric(lgrid^2)
seq_len(lgrid^2)
## --- Loop over target h* ----------------------------------------------------
for (i in seq_len(lgrid^2)) {
print(i)
h_star_0 <- grid_matrix[i, 1]
h_star_1 <- grid_matrix[i, 2]
# h*(z|w) as a function in (z,w)
h_star_fun <- function(z, W) z * make_h(W, h_star_0, h_star_1) +
(1 - z) * (1 - make_h(W, h_star_0, h_star_1))
# Initial Q
Qn0 <- function(Znew, Win_new) Q_from_fit(Znew, Win_new)
# Clever covariate weights
weights <- h_star_fun(Z, W) / hfun(Z, hw)
# One-dimensional fluctuation (TMLE)
offset_eta <- qlogis(Qn0(Z, Win))
fluctuation <- glm(Y ~ offset(offset_eta) + 1, weights = weights,
family = binomial)
eps <- as.numeric(coef(fluctuation))
# Updated Q
Qn <- (function(Qold, eps_) {
function(Znew, Win_new) plogis(qlogis(Qold(Znew, Win_new)) + eps_)
})(Qn0, eps)
# Parameter map G(Q; h*)
G_comp_z <- function(Q_model, h0, h1) {
hW <- make_h(W, h0, h1)
Q0 <- Q_model(rep(0, n), Win)
Q1 <- Q_model(rep(1, n), Win)
mean(Q0 * (1 - hW) + Q1 * hW)
}
psi[i] <- G_comp_z(Qn, h_star_0, h_star_1)
# EIF & std bound
terms <- calc_plugin_eif_terms(Qn, Y, Z, Win, W, h_star_fun, hw)
eif   <- terms$clever_term + terms$plugin_term - psi[i]
sdbound[i] <- sqrt(var(eif) / n)
}
warnings()
list(
grid      = grid_matrix,
psi       = psi,
sdbound   = sdbound
)
psi[13]*100
sdbound[13]*100
psi[88]*100
sdbound[88]*100
psi[1]*100
sdbound[1]*100
grid_matrix[13,]
grid_matrix[88,]
grid_matrix[1,]
grid_matrix[88,]
(mean(Y[Z==1])-mean(Y[Z==0]))/(mean(A[Z==1])-mean(A[Z==0]))
gmat[2,1]*hstr0
grid_matrix[13,]
set.seed(123)
setwd("/Users/cgmeixide/Projects/implied_interventions")
source("data.R")
data <- medicare_loader(4)
Wall <- data[, c("W.zip_msa_list", "W.birthyear_list", "W.female_list", "W.dep_dx_0m")]
W    <- data$W.dep_dx_0m
Z    <- data$treatment
A    <- data$approved_app
Yprim <- data[, 1]
Y    <- ifelse(Yprim == 0, 0, 1)
## --- HAL helpers ------------------------------------------------------------
# Minimal reimplementation to build design matrices for new data given basis list
make_newx_from_basis <- function(basis_list, X_new) {
X_new <- as.matrix(X_new)
n <- nrow(X_new)
B <- matrix(NA_real_, nrow = n, ncol = length(basis_list))
for (j in seq_along(basis_list)) {
info <- basis_list[[j]]
ind <- rep(1, n)
for (k in seq_along(info$cols)) {
ind <- ind * (X_new[, info$cols[k]] >= info$cutoffs[k])
}
B[, j] <- ind
}
B
}
## --- Prob helpers -----------------------------------------------------------
make_h <- function(w, h0_0, h0_1) ifelse(w == 0, h0_0, h0_1)
hfun   <- function(z, f) z * f + (1 - z) * (1 - f)  # P(Z=z | W) in 1-dim form
## --- Libraries --------------------------------------------------------------
library(hal9001)
library(glmnet)
## --- Bases & fits -----------------------------------------------------------
Win <- as.matrix(Wall)
n   <- nrow(Win)
# h(z|w)
hal_w <- enumerate_basis(x = Win, max_degree = 2)
HW    <- make_design_matrix(Win, hal_w, p_reserve = 1)
fitcvw <- cv.glmnet(HW, Z, family = "binomial", intercept = FALSE, standardize = FALSE)
fitw   <- glmnet(HW, Z, family = "binomial", lambda = fitcvw$lambda.min,
intercept = FALSE, standardize = FALSE)
hw <- as.numeric(predict(fitw, newx = HW, type = "response"))
# Q(z,w) = E[Y|Z,W]
hal_zw <- enumerate_basis(x = cbind(Z, Win), max_degree = 2)
HZW    <- make_design_matrix(cbind(Z, Win), hal_zw, p_reserve = 1)
fitcvy <- cv.glmnet(HZW, Y, family = "binomial", intercept = FALSE, standardize = FALSE)
fity   <- glmnet(HZW, Y, family = "binomial", lambda = fitcvy$lambda.min,
intercept = FALSE, standardize = FALSE)
# Wrapper producing Q(Z,W) on demand (compatible with offset fluctuation)
Q_from_fit <- function(Znew, Win_new) {
Xnew <- cbind(Znew, Win_new)
Hnew <- make_newx_from_basis(hal_zw, Xnew)
as.numeric(predict(fity, newx = Hnew, type = "response"))
}
## --- EIF components ---------------------------------------------------------
calc_plugin_eif_terms <- function(Q_model, Y, Z, Win, W, h_star_fun, hw_vec) {
Q_hat   <- Q_model(Z, Win)
clever  <- (h_star_fun(Z, W) / hfun(Z, hw_vec)) * (Y - Q_hat)
Q0_hat  <- Q_model(rep(0, length(Z)), Win)
Q1_hat  <- Q_model(rep(1, length(Z)), Win)
plugin  <- Q0_hat * (1 - h_star_fun(W = W, z = 1)) + Q1_hat * h_star_fun(W = W, z = 1)
list(clever_term = clever, plugin_term = plugin)
}
## --- Grid of target h* ------------------------------------------------------
lgrid <- 10
grid_matrix <- as.matrix(expand.grid(seq(0, 1, length.out = lgrid),
seq(0, 1, length.out = lgrid)))
colnames(grid_matrix) <- c("h_star_0", "h_star_1")
psi     <- numeric(lgrid^2)
sdbound <- numeric(lgrid^2)
## --- Loop over target h* ----------------------------------------------------
for (i in seq_len(lgrid^2)) {
print(i)
h_star_0 <- grid_matrix[i, 1]
h_star_1 <- grid_matrix[i, 2]
# h*(z|w) as a function in (z,w)
h_star_fun <- function(z, W) z * make_h(W, h_star_0, h_star_1) +
(1 - z) * (1 - make_h(W, h_star_0, h_star_1))
# Initial Q
Qn0 <- function(Znew, Win_new) Q_from_fit(Znew, Win_new)
# Clever covariate weights
weights <- h_star_fun(Z, W) / hfun(Z, hw)
# One-dimensional fluctuation (TMLE)
offset_eta <- qlogis(Qn0(Z, Win))
fluctuation <- glm(Y ~ offset(offset_eta) + 1, weights = weights,
family = binomial)
eps <- as.numeric(coef(fluctuation))
# Updated Q
Qn <- (function(Qold, eps_) {
function(Znew, Win_new) plogis(qlogis(Qold(Znew, Win_new)) + eps_)
})(Qn0, eps)
# Parameter map G(Q; h*)
G_comp_z <- function(Q_model, h0, h1) {
hW <- make_h(W, h0, h1)
Q0 <- Q_model(rep(0, n), Win)
Q1 <- Q_model(rep(1, n), Win)
mean(Q0 * (1 - hW) + Q1 * hW)
}
psi[i] <- G_comp_z(Qn, h_star_0, h_star_1)
# EIF & std bound
terms <- calc_plugin_eif_terms(Qn, Y, Z, Win, W, h_star_fun, hw)
eif   <- terms$clever_term + terms$plugin_term - psi[i]
sdbound[i] <- sqrt(var(eif) / n)
}
psi[13]*100
sdbound[13]*100
psi[88]*100
sdbound[88]*100
hstr0=grid_matrix[88,1]*100
hstr1=grid_matrix[88,2]*100
gmat[2,1]*hstr0
gmat[2,2]*hstr1
psi[1]*100
sdbound[1]*100
# Implied intervention 1
gmat[2,1]*grid_matrix[88,1]*100
gmat[2,2]*grid_matrix[88,2]*100
# Implied intervention 1
gmat[2,1]*grid_matrix[13,1]*100
gmat[2,2]*grid_matrix[13,2]*100
# =============================================
# EM-HAL for KL Projection of g* onto {g(h)}
# =============================================
set.seed(1)
# --- Libraries
library(hal9001)
library(glmnet)
# --- Switches
use_simulated_data <- FALSE   # TRUE = synthetic; FALSE = real (needs data.R / medicare_loader)
sim_discrete       <- TRUE    # for synthetic: A binary vs continuous
max_iters          <- 100
tol                <- 1e-4    # convergence tolerance on log-likelihood
# --- Desired intervention g*(A|W): only parameter(s) you set
pistar <- 0.40     # if A is binary, P(A=1|W) = pistar (constant here)
mustar <- 0.0      # if A is continuous, mean for N(mustar, sdstar^2)
sdstar <- 0.5
# =============================================
# Helpers for HAL design on W
# =============================================
make_hal_design <- function(W) {
basis_list <- enumerate_basis(
x = W,
max_degree = NULL,
smoothness_orders = rep(0, ncol(W)),
include_zero_order = TRUE,
include_lower_order = TRUE
)
H <- make_design_matrix(W, basis_list)
list(H = as.matrix(H), basis = basis_list)
}
make_newx_from_basis <- function(basis_list, W_new) {
make_design_matrix(as.matrix(W_new), basis_list)
}
# =============================================
# Load data & define \hat p(A|Z,W)
# =============================================
# --- Synthetic data
n <- 1000
p <- 2
gamma <- 2
sdnoise <- 0.2
# --- Synthetic data
n <- 1000
p <- 2
gamma <- 2
sdnoise <- 0.2
# Covariates
W <- matrix(runif(n * p, -2, 2), nrow = n, ncol = p)
# Instrument Z ~ Bernoulli(plogis(ezw(W)))
ezw <- function(W) W[,1] * sqrt(abs(W[,2])) * sign(W[,2])
Z <- rbinom(n, 1, plogis(ezw(W)))
# Treatment A | Z,W (binary or continuous)
if (sim_discrete) {
# Logistic in Z (no W used here to keep hat{p} simple/known)
A <- rbinom(n, 1, plogis(gamma * Z))
pawz <- function(a, w, z) {           # \hat p(A=a|Z=z,W=w)
p1 <- plogis(gamma * z)
a * p1 + (1 - a) * (1 - p1)
}
# Desired g*: Bernoulli(pistar) independent of W
Astar <- rbinom(n, 1, pistar)
gstar <- function(a, w) a * pistar + (1 - a) * (1 - pistar)
} else {
# Continuous A | Z,W ~ N(mu(Z,W), sdnoise^2)
eazw <- function(z, W) gamma * z + sin(W[,1]) * log(1 + W[,2]^2)
mu <- eazw(Z, W)
A <- rnorm(n, mean = mu, sd = sdnoise)
pawz <- function(a, w, z) {           # \hat p(A=a|Z=z,W=w)
m <- gamma * z + sin(w[1]) * log(1 + w[2]^2)
dnorm(a, mean = m, sd = sdnoise)
}
# Desired g*: N(mustar, sdstar^2) independent of W
Astar <- rnorm(n, mean = mustar, sd = sdstar)
gstar <- function(a, w) dnorm(a, mean = mustar, sd = sdstar)
}
# =============================================
# Build HAL design for h(Z=1|W) (logistic link)
# =============================================
hal_W <- make_hal_design(W)
X <- hal_W$H
basis_W <- hal_W$basis
# =============================================
# EM Initialization: h^{(0)}(1|W) (start at 0.5)
# =============================================
h_curr <- rep(0.5, n)
# Utility: observed-data log-likelihood for current h
loglik_obs <- function(h_vec) {
# \ell(h) = sum_i log( p(A_i^*|0,W_i)*(1-h_i) + p(A_i^*|1,W_i)*h_i )
denom <- mapply(function(ai, wi, h) {
p0 <- pawz(ai, wi, z = 0)
p1 <- pawz(ai, wi, z = 1)
p0 * (1 - h) + p1 * h
}, Astar, split(W, row(W)), h_vec)
sum(log(pmax(denom, .Machine$double.eps)))
}
# Duplication trick to fit the M-step with glmnet
duplication_for_glmnet <- function(W, tau, basis_W) {
n <- nrow(W)
X <- make_newx_from_basis(basis_W, W)
X2 <- rbind(X, X)
y2 <- c(rep(1L, n), rep(0L, n))
w2 <- c(tau, 1 - tau)
list(X = X2, y = y2, w = w2)
}
# =============================================
# EM Loop
# =============================================
ll_prev <- -Inf
max_iters
max_iters          <- 3
# --- E-step: tau_i = P_k(Z=1 | A_i^*, W_i)
tau <- mapply(function(ai, wi, h) {
num <- pawz(ai, wi, z = 1) * h
den <- pawz(ai, wi, z = 0) * (1 - h) + pawz(ai, wi, z = 1) * h
num / pmax(den, .Machine$double.eps)
}, Astar, split(W, row(W)), h_curr)
# --- M-step: maximize Q via HAL/logistic with duplication trick
dup <- duplication_for_glmnet(W, tau, basis_W)
cv_fit <- cv.glmnet(dup$X, dup$y, family = "binomial",
weights = dup$w, intercept = FALSE, standardize = FALSE)
fit <- glmnet(dup$X, dup$y, family = "binomial",
lambda = cv_fit$lambda.min, weights = dup$w,
intercept = FALSE, standardize = FALSE)
# Update h^{(k+1)}(1|W_i)
XW <- make_newx_from_basis(basis_W, W)
h_next <- as.numeric(predict(fit, newx = XW, type = "response"))
# Check convergence using observed-data log-likelihood
ll_curr <- loglik_obs(h_next)
if (k > 0 && abs(ll_curr - ll_prev) < tol) {
message(sprintf("Converged at iteration %d: Δll=%.3e", k, ll_curr - ll_prev))
h_curr <- h_next
break
}
k=1
# --- E-step: tau_i = P_k(Z=1 | A_i^*, W_i)
tau <- mapply(function(ai, wi, h) {
num <- pawz(ai, wi, z = 1) * h
den <- pawz(ai, wi, z = 0) * (1 - h) + pawz(ai, wi, z = 1) * h
num / pmax(den, .Machine$double.eps)
}, Astar, split(W, row(W)), h_curr)
# --- M-step: maximize Q via HAL/logistic with duplication trick
dup <- duplication_for_glmnet(W, tau, basis_W)
cv_fit <- cv.glmnet(dup$X, dup$y, family = "binomial",
weights = dup$w, intercept = FALSE, standardize = FALSE)
fit <- glmnet(dup$X, dup$y, family = "binomial",
lambda = cv_fit$lambda.min, weights = dup$w,
intercept = FALSE, standardize = FALSE)
# Update h^{(k+1)}(1|W_i)
XW <- make_newx_from_basis(basis_W, W)
h_next <- as.numeric(predict(fit, newx = XW, type = "response"))
# Check convergence using observed-data log-likelihood
ll_curr <- loglik_obs(h_next)
if (k > 0 && abs(ll_curr - ll_prev) < tol) {
message(sprintf("Converged at iteration %d: Δll=%.3e", k, ll_curr - ll_prev))
h_curr <- h_next
break
}
h_curr <- h_next
ll_prev <- ll_curr
if (k == max_iters) {
message("Reached max_iters without hitting tol; returning last iterate.")
}
h_hat <- h_curr
# 2) Distribution of h_hat(W)
hist(em_hal_result$h_hat, breaks = 30, main = "Distribution of h_hat(W)",
xlab = "h_hat(W) = P(Z=1 | W) under KL-projected model")
# =============================================
# Outputs
# =============================================
# h_hat(W) = P(Z=1 | W) under the KL-projection g(h^\dagger)
h_hat <- h_curr
# A compact list you can save/return:
em_hal_result <- list(
h_hat = h_hat,
basis_W = basis_W,
# helper to predict h_hat at new W:
predict_h = function(Wnew) {
Xnew <- make_newx_from_basis(basis_W, as.matrix(Wnew))
as.numeric(predict(fit, newx = Xnew, type = "response"))
},
loglik = ll_prev,
pistar = pistar,
use_simulated_data = use_simulated_data
)
# Quick sanity prints
cat(sprintf("Final observed-data log-likelihood: %.4f\n", em_hal_result$loglik))
cat(sprintf("Mean h_hat(W): %.3f\n", mean(em_hal_result$h_hat)))
# 2) Distribution of h_hat(W)
hist(em_hal_result$h_hat, breaks = 30, main = "Distribution of h_hat(W)",
xlab = "h_hat(W) = P(Z=1 | W) under KL-projected model")
# 3) If original Z is available (sim or real): calibration of h_hat vs Z
if (exists("Z")) {
brks <- quantile(em_hal_result$h_hat, probs = seq(0, 1, by = 0.1), na.rm = TRUE)
grp <- cut(em_hal_result$h_hat, breaks = unique(brks), include.lowest = TRUE)
agg <- aggregate(
data.frame(h = em_hal_result$h_hat, Z = Z),
by = list(bin = grp),
FUN = mean
)
plot(agg$h, agg$Z, pch = 16,
xlab = "Mean predicted h_hat by decile",
ylab = "Empirical mean of Z",
main = "Calibration: E[Z | h_hat] vs h_hat")
abline(0, 1, lty = 2)
}
# 4) Simulated-data density check: A vs A* (target) vs model-implied A~g(h^dagger)
if (use_simulated_data) {
# Simulate A_tilde from implied mixture g(h)(A|W) = sum_z p(A|z,W) h_hat(z|W)
# For binary A: sample Z~Bern(h_hat), then A~Bern(p1(z,W))
# For continuous A: sample Z~Bern(h_hat), then A~N(mu(z,W), sdnoise^2)
Z_tilde <- rbinom(n, 1, em_hal_result$h_hat)
if (sim_discrete) {
# Sample A|Z,W ~ Bernoulli(plogis(gamma Z)) to match pawz()
p1 <- plogis(gamma * Z_tilde)
A_tilde <- rbinom(n, 1, p1)
# Overlay proportions (kernel density on {0,1} is not helpful; show barplots)
op <- par(mfrow = c(1, 3))
barplot(table(A) / length(A), ylim = c(0, 1), main = "Empirical A", ylab = "Probability")
barplot(table(Astar) / length(Astar), ylim = c(0, 1), main = "Target A* ~ g*", ylab = "Probability")
barplot(table(A_tilde) / length(A_tilde), ylim = c(0, 1), main = "Model-implied A ~ g(h^dagger)", ylab = "Probability")
par(op)
} else {
# Continuous case: need mu(z,W) and sdnoise from simulation setup
mu_tilde <- gamma * Z_tilde + sin(W[,1]) * log(1 + W[,2]^2)
A_tilde <- rnorm(n, mean = mu_tilde, sd = sdnoise)
# Density overlays
rng <- range(A, Astar, A_tilde)
plot(density(A), xlim = rng, ylim = c(0, max(density(A)$y, density(Astar)$y, density(A_tilde)$y)),
main = "Densities: A, A*, A~g(h^dagger)", xlab = "A")
lines(density(Astar), lty = 2)
lines(density(A_tilde))
legend("topright", legend = c("Empirical A", "Target A*", "Model-implied A"),
lty = c(1, 2, 1), col = c("black", "black", "gray50"), bty = "n")
}
}
# Continuous case: need mu(z,W) and sdnoise from simulation setup
mu_tilde <- gamma * Z_tilde + sin(W[,1]) * log(1 + W[,2]^2)
# Continuous case: need mu(z,W) and sdnoise from simulation setup
mu_tilde <- gamma * Z_tilde + sin(W[,1]) * log(1 + W[,2]^2)
# Continuous case: need mu(z,W) and sdnoise from simulation setup
mu_tilde <- gamma * Z_tilde + sin(W[,1]) * log(1 + W[,2]^2)
# 5) 3D surface of h_hat(W) when W is 2D (optional)
threed <- FALSE  # set TRUE to enable
if (threed && ncol(W) == 2) {
if (requireNamespace("plotly", quietly = TRUE)) {
library(plotly)
W1_seq <- seq(min(W[,1]), max(W[,1]), length.out = 80)
W2_seq <- seq(min(W[,2]), max(W[,2]), length.out = 80)
grd <- expand.grid(W1 = W1_seq, W2 = W2_seq)
h_grid <- em_hal_result$predict_h(as.matrix(grd))
H <- matrix(h_grid, nrow = length(W1_seq), byrow = FALSE)
plot_ly(x = W1_seq, y = W2_seq, z = ~H) |>
add_surface(showscale = FALSE) |>
layout(scene = list(
xaxis = list(title = "W1"),
yaxis = list(title = "W2"),
zaxis = list(title = "h_hat"),
aspectratio = list(x = 1, y = 1, z = 0.6)
))
} else {
message("plotly not installed; skipping 3D surface.")
}
}
